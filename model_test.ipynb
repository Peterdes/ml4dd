{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polaris as po\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = po.load_benchmark(\"polaris/pkis1-kit-wt-mut-c-1\")\n",
    "df = benchmark.dataset.table\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[['KIT_(T6701_mutant)', 'KIT_(V560G_mutant)', 'KIT']]\n",
    "y_test = test_df[['KIT_(T6701_mutant)', 'KIT_(V560G_mutant)', 'KIT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['SMILES', 'KIT_(T6701_mutant)', 'KIT_(V560G_mutant)', 'KIT', 'CLASS_KIT_(T6701_mutant)', 'CLASS_KIT_(V560G_mutant)', 'CLASS_KIT']).values\n",
    "X_test = test_df.drop(columns=['SMILES','KIT_(T6701_mutant)', 'KIT_(V560G_mutant)', 'KIT', 'CLASS_KIT_(T6701_mutant)', 'CLASS_KIT_(V560G_mutant)', 'CLASS_KIT']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = benchmark.get_train_test_split()\n",
    "ys = train.y\n",
    "ys = np.stack([ys[target] for target in benchmark.target_cols], axis=1)\n",
    "mask = ~np.any(np.isnan(ys), axis=1)\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'Support Vector Classifier': SVC(probability=True),\n",
    "    'K-Neighbors Classifier': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Function to calculate PR-AUC\n",
    "def calculate_pr_auc(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "target_variables = ['CLASS_KIT_(T6701_mutant)', 'CLASS_KIT_(V560G_mutant)', 'CLASS_KIT']\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_results = {}\n",
    "    for target in target_variables:\n",
    "        y_train_target = y_train[target]\n",
    "        y_test_target = y_test[target]\n",
    "\n",
    "        # Cross-validated predictions\n",
    "        y_scores = cross_val_predict(model, X_train, y_train_target, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "        # Fit the model on the entire training set\n",
    "        model.fit(X_train, y_train_target)\n",
    "\n",
    "        # Predict probabilities on the test set\n",
    "        y_test_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate PR-AUC\n",
    "        pr_auc = calculate_pr_auc(y_test_target, y_test_scores)\n",
    "\n",
    "        # Store the result\n",
    "        model_results[target] = pr_auc\n",
    "\n",
    "    results[name] = model_results\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def tanimoto_dist(a, b):\n",
    "    dotprod = np.dot(a, b)\n",
    "    return 1.0 - (dotprod / (np.sum(a) + np.sum(b) - dotprod))\n",
    "\n",
    "# Initialize KNN classifier with Tanimoto distance\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, metric=tanimoto_dist)\n",
    "\n",
    "# Train the model\n",
    "knn_clf.fit(X_train[mask], y_train.values[mask])\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "\n",
    "y_prob = knn_clf.predict_proba(X_test)\n",
    "y_prob = np.stack(y_prob, axis=1)\n",
    "\n",
    "y_pred = {k: y_pred[:, idx] for idx, k in enumerate(benchmark.target_cols)}\n",
    "y_prob = {k: y_prob[:, idx, 1] for idx, k in enumerate(benchmark.target_cols)}\n",
    "\n",
    "benchmark.evaluate(y_pred=y_pred, y_prob=y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize the XGBRegressor model\n",
    "xgb_reg = XGBRegressor(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_reg.fit(X_train[mask], y_train[mask])\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "# Inverse the sigmoid transformation\n",
    "y_prob = np.stack([1-y_pred, y_pred], axis=2)\n",
    "\n",
    "y_pred = y_pred > 0.5\n",
    "\n",
    "# Convert the predictions to a dictionary\n",
    "y_pred = {k: y_pred[:, idx] for idx, k in enumerate(benchmark.target_cols)}\n",
    "y_prob = {k: y_prob[:, idx, 1] for idx, k in enumerate(benchmark.target_cols)}\n",
    "\n",
    "results = benchmark.evaluate(y_pred=y_pred, y_prob=y_prob)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "hyper_params = {\n",
    "    'task_type': 'CPU',  # Use 'GPU' if you have GPU available\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'learning_rate': 0.001,\n",
    "    'iterations': 10000,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'rsm': 0.9,  # equivalent to feature_fraction\n",
    "    'subsample': 0.7,  # equivalent to bagging_fraction\n",
    "    'bagging_temperature': 1,\n",
    "    'border_count': 512,  # equivalent to max_bin\n",
    "    'verbose': 2,\n",
    "    'thread_count': -1  # use all available CPU cores\n",
    "}\n",
    "\n",
    "# Create the base CatBoost model\n",
    "model = CatBoostRegressor(**hyper_params)\n",
    "\n",
    "# Wrap it with MultiOutputRegressor for multi-output regression\n",
    "model = MultiOutputRegressor(model)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train[mask], y_train[mask])\n",
    "\n",
    "y_pred = np.exp(model.predict(X_test))\n",
    "y_prob = np.stack([1-y_pred, y_pred], axis=2)\n",
    "y_pred = y_pred > 0.5\n",
    "\n",
    "y_pred = {k: y_pred[:, idx] for idx, k in enumerate(benchmark.target_cols)}\n",
    "y_prob = {k: y_prob[:, idx, 1] for idx, k in enumerate(benchmark.target_cols)}\n",
    "\n",
    "results = benchmark.evaluate(y_pred=y_pred, y_prob=y_prob)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
